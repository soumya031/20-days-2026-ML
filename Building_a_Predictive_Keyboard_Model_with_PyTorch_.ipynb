{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Every time you type on your smartphone, you see three words pop up as suggestions. That’s a predictive keyboard in action. These suggestions aren’t random. They are based on deep learning models that have learned language patterns from tons of text data. So, if you want to learn about building the model behind a predictive keyboard, this article is for you. In this article, I’ll take you through the task of building a predictive keyboard model with PyTorch.\n",
        "\n",
        "**Building a Predictive Keyboard Model Using PyTorch**\n",
        "\n",
        "The task of building a predictive keyboard model includes these steps:\n",
        "\n",
        "1. Tokenizing and preparing natural language data\n",
        "2. Building a vocabulary and converting words to indices\n",
        "3. Training a next-word prediction model using LSTMs\n",
        "4. Generating top-k predictions like a predictive keyboard\n",
        "\n",
        "We will use these steps, and in the end, we will see a model generating three suggestions for the next word, just like a predictive keyboard in your smartphone.\n",
        "\n",
        "The richer the data, the better your model will generalize. So, the dataset we will use is based on the stories of Sherlock Holmes. You can find this dataset [here](https:///content/sherlock-holm.es_stories_plain-text_advs.txt)"
      ],
      "metadata": {
        "id": "_7rBDH4hiwZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Preparing the Dataset\n",
        "\n",
        "We will start with tokenizing the text data and converting everything to lowercase:"
      ],
      "metadata": {
        "id": "1QoNL-Asj1Uq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1526d6bf"
      },
      "source": [
        "\n",
        "Build a predictive keyboard model using PyTorch by processing the text data from \"/content/sherlock-holm.es_stories_plain-text_advs.txt\". The task includes building a vocabulary from the Sherlock Holmes stories, preparing input-output sequences for training, defining and training an LSTM-based next-word prediction model, and implementing a function to provide top-3 word suggestions based on a given input string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1495a90f",
        "outputId": "d9219131-5eec-49f2-a9ca-20813d66f7e7"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# load data using the correct absolute path\n",
        "file_path = '/content/sherlock-holm.es_stories_plain-text_advs.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Total Tokens:\", len(tokens))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 125772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1585f467"
      },
      "source": [
        "**Step 2: Creating a Vocabulary**\n",
        "\n",
        "Next, we need a way to convert words into numbers. So we will create:\n",
        "\n",
        "1. a dictionary to map each word to an index\n",
        "2. and another dictionary to reverse it back\n",
        "\n",
        "So, let’s build the vocabulary and create word-to-index mappings:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39066946",
        "outputId": "3738ee43-5c53-4d4b-fa74-6f5ac8308a25"
      },
      "source": [
        "# Step 2: Build vocabulary and convert words to indices\n",
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "# Create a vocabulary mapping from word to index\n",
        "word_counts = Counter(tokens)\n",
        "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f\"Unique tokens (Vocab Size): {vocab_size}\")\n",
        "\n",
        "# Convert tokens to indices\n",
        "encoded_text = [word_to_idx[word] for word in tokens]\n",
        "print(f\"Encoded text length: {len(encoded_text)}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens (Vocab Size): 8360\n",
            "Encoded text length: 125772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "910726b2"
      },
      "source": [
        "**Step 3: Building Input-Output Sequences**\n",
        "\n",
        "To predict the next word, the model needs context. We can use a sliding window approach. So, let’s create input-target sequences for next word prediction:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbfc8851",
        "outputId": "9234c72a-7452-4602-867a-437fb91521d1"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Define sequence length\n",
        "seq_length = 10\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Create sliding window sequences\n",
        "for i in range(len(encoded_text) - seq_length):\n",
        "    X.append(encoded_text[i:i + seq_length])\n",
        "    y.append(encoded_text[i + seq_length])\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.long)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "print(f\"Input tensor shape: {X_tensor.shape}\")\n",
        "print(f\"Target tensor shape: {y_tensor.shape}\")\n",
        "\n",
        "# Create dataset and verify\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "print(f\"Total number of samples: {len(dataset)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape: torch.Size([125762, 10])\n",
            "Target tensor shape: torch.Size([125762])\n",
            "Total number of samples: 125762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8e2d4c2"
      },
      "source": [
        "**Step 4: Designing the Model Architecture**\n",
        "\n",
        "For sequence data, LSTMs are still the go-to. They can remember patterns across time steps, which makes them perfect for language modelling. So, let’s define the LSTM-based Predictive Keyboard model:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b27fc5cc",
        "outputId": "5a85fa66-fb97-4481-c2b3-c8ca493809f2"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Define the LSTM model architecture\n",
        "class PredictiveLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(PredictiveLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_length)\n",
        "        embeds = self.embedding(x)\n",
        "        # lstm_out shape: (batch_size, seq_length, hidden_dim)\n",
        "        lstm_out, (h_n, c_n) = self.lstm(embeds)\n",
        "        # Use the hidden state of the last time step\n",
        "        out = self.fc(lstm_out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# 2. Initialize model and hyperparameters\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "model = PredictiveLSTM(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "# 3. Create DataLoader\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 4. Define Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 5. Training Loop\n",
        "num_epochs = 5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Starting training on {device}...\")\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % 200 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] completed. Average Loss: {avg_loss:.4f}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training on cuda...\n",
            "Epoch [1/5], Step [200/983], Loss: 6.3324\n",
            "Epoch [1/5], Step [400/983], Loss: 5.4437\n",
            "Epoch [1/5], Step [600/983], Loss: 5.7752\n",
            "Epoch [1/5], Step [800/983], Loss: 5.3442\n",
            "Epoch [1/5] completed. Average Loss: 5.6032\n",
            "Epoch [2/5], Step [200/983], Loss: 5.0917\n",
            "Epoch [2/5], Step [400/983], Loss: 4.9217\n",
            "Epoch [2/5], Step [600/983], Loss: 5.1580\n",
            "Epoch [2/5], Step [800/983], Loss: 4.5925\n",
            "Epoch [2/5] completed. Average Loss: 4.7676\n",
            "Epoch [3/5], Step [200/983], Loss: 4.4735\n",
            "Epoch [3/5], Step [400/983], Loss: 4.4755\n",
            "Epoch [3/5], Step [600/983], Loss: 4.1851\n",
            "Epoch [3/5], Step [800/983], Loss: 3.9924\n",
            "Epoch [3/5] completed. Average Loss: 4.3356\n",
            "Epoch [4/5], Step [200/983], Loss: 3.6541\n",
            "Epoch [4/5], Step [400/983], Loss: 3.9229\n",
            "Epoch [4/5], Step [600/983], Loss: 3.6778\n",
            "Epoch [4/5], Step [800/983], Loss: 3.5726\n",
            "Epoch [4/5] completed. Average Loss: 3.9525\n",
            "Epoch [5/5], Step [200/983], Loss: 3.4566\n",
            "Epoch [5/5], Step [400/983], Loss: 3.7921\n",
            "Epoch [5/5], Step [600/983], Loss: 3.6637\n",
            "Epoch [5/5], Step [800/983], Loss: 3.5912\n",
            "Epoch [5/5] completed. Average Loss: 3.5923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "426a8b24"
      },
      "source": [
        "## Implement Top-K Prediction\n",
        "\n",
        "Create a function to predict the top 3 most likely next words for a given input string using the trained LSTM model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14b3d1d4"
      },
      "source": [
        "### Predictive Keyboard Workflow Diagram\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Raw Text Data: Sherlock Holmes] --> B[Step 1: Tokenization & Preprocessing]\n",
        "    B --> C[Convert to Lowercase & Word Tokenize]\n",
        "    C --> D[Step 2: Build Vocabulary]\n",
        "    D --> E[Map Unique Words to Indices]\n",
        "    E --> F[Step 3: Prepare Sequences]\n",
        "    F --> G[Sliding Window: 10 Words Input + 1 Target]\n",
        "    G --> H[Step 4: Model Architecture]\n",
        "    H --> I[Embedding Layer -> LSTM -> Linear Layer]\n",
        "    I --> J[Step 5: Training Loop]\n",
        "    J --> K[Loss: CrossEntropy, Optimizer: Adam]\n",
        "    K --> L[Step 6: Top-K Prediction Function]\n",
        "    L --> M[Input Text -> Softmax -> Top 3 Suggestions]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "384ac9c5",
        "outputId": "6a2e0a52-ae48-4405-f55f-b4cf30a514c7"
      },
      "source": [
        "def predict_next_words(input_text, model, word_to_idx, idx_to_word, seq_length=10):\n",
        "    model.eval()\n",
        "    tokens = word_tokenize(input_text.lower())\n",
        "    indices = [word_to_idx.get(w, 0) for w in tokens]\n",
        "    if len(indices) < seq_length:\n",
        "        indices = [0] * (seq_length - len(indices)) + indices\n",
        "    else:\n",
        "        indices = indices[-seq_length:]\n",
        "    input_tensor = torch.tensor([indices], dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        top_k_probs, top_k_indices = torch.topk(probabilities, 3)\n",
        "    predictions = [idx_to_word[idx.item()] for idx in top_k_indices[0]]\n",
        "    return predictions\n",
        "\n",
        "test_input = \"the adventure of the\"\n",
        "suggestions = predict_next_words(test_input, model, word_to_idx, idx_to_word)\n",
        "print(f\"Input: '{test_input}'\")\n",
        "print(f\"Top 3 Suggestions: {suggestions}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 'the adventure of the'\n",
            "Top 3 Suggestions: ['man', 'most', 'coronet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41e9a97d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "\n",
        "**How was the text data processed for the LSTM model?**  \n",
        "The Sherlock Holmes text was tokenized using NLTK into 125,772 tokens and converted to lowercase. A vocabulary of 8,360 unique words was created, mapping each word to a numerical index. The data was then organized into sliding windows of 10 words each to create 125,762 input-output training samples.\n",
        "\n",
        "**What is the architecture of the predictive model?**  \n",
        "The model is a PyTorch-based LSTM consisting of an Embedding layer (100 dimensions), an LSTM layer (256 hidden dimensions), and a Linear layer that maps the output back to the vocabulary size ($8,360$) for word prediction.\n",
        "\n",
        "**How does the predictive keyboard functionality work?**  \n",
        "A custom function takes an input string, tokenizes and pads/truncates it to a length of 10, and passes it through the trained model. The model generates logits, which are converted to probabilities via Softmax, and the `torch.topk` function extracts the 3 most likely next words.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Dataset Scale**: The corpus yielded **125,772 total tokens** and a manageable vocabulary of **8,360 unique words**, providing a solid foundation for sequential learning.\n",
        "*   **Training Efficiency**: The model demonstrated effective learning over 5 epochs, with the average loss dropping significantly from **5.60 to 3.59**.\n",
        "*   **Predictive Performance**: For the sample input *\"the adventure of the\"*, the model successfully suggested relevant continuations: **['man', 'most', 'coronet']**, reflecting themes found in the Sherlock Holmes stories.\n",
        "*   **Hardware Acceleration**: Utilizing GPU (`cuda`) allowed for efficient processing of the 125,762 samples using a batch size of 128.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Model Refinement**: To improve the coherence of predictions, increasing the number of training epochs or adding a second LSTM layer could help capture deeper semantic relationships in the Victorian-era prose.\n",
        "*   **Handling Out-of-Vocabulary (OOV) Words**: The current implementation defaults unknown words to index 0; implementing a specific `<UNK>` token strategy or character-level embedding could improve robustness for modern user inputs not found in the original text.\n"
      ]
    }
  ]
}